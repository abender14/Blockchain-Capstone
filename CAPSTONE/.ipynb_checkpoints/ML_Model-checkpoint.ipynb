{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"cs-training.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"cs-test.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_df = train.describe()\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print structure of data\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan of Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip whitespace from entire data frame\n",
    "train = train.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with central tendency metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#are the any NA values?\n",
    "train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which columns have missing values\n",
    "train.isnull().sum()[train.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are missing values in only two features: MonthlyIncomeand NumberOfDependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#what percentage of records are missing\n",
    "train.isnull().sum()[train.isnull().sum()>0]/len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MonthlyIncome has nearly 20% of it's data as missing. This is quote high and brings us to a decision point. We can drop this column entirely, drop the rows containing missing values, or impute the missing values. SimpleImputation may be subject to error since it would be applying a central tendency value to all 29731 of these rows; however, imputation is important since Machine Learning algorithms don't like missing values. We can also impute using a sophisticated, predictive method. \n",
    "\n",
    "From subject matter context, we know that monthly income is a feature important in our dataset and will elect to keep it. Therefore, we will be doing imputation for these missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_median = train_full['MonthlyIncome'].median()\n",
    "\n",
    "#REMEMBER TO TRY ITERATIVE IMPUTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO WE NEED TO TAKE MEDIAN FROM NON NA ROWS ONLY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['MonthlyIncome'].fillna(income_median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['MonthlyIncome'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more missing values in Monthly Income! Now NumberOfDependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependents_median = train_full['NumberOfDependents'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['NumberOfDependents'].fillna(dependents_median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['NumberOfDependents'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more missing values in NumberOfDependents! Now onto Outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check outliers\n",
    "df = train_full\n",
    "f, axes = plt.subplots(3, 2, figsize=(10, 10), sharex=False)\n",
    "sns.distplot( df[\"NumberOfDependents\"] , color=\"olive\", ax=axes[0, 0])\n",
    "sns.distplot( df[\"NumberOfTime30-59DaysPastDueNotWorse\"] , color=\"green\", ax=axes[0, 1])\n",
    "sns.distplot( df[\"NumberOfOpenCreditLinesAndLoans\"] , color=\"red\", ax=axes[1, 0])\n",
    "sns.distplot( df[\"NumberOfTimes90DaysLate\"] , color=\"blue\", ax=axes[1, 1])\n",
    "sns.distplot( df[\"NumberRealEstateLoansOrLines\"] , color=\"teal\", ax=axes[2, 0])\n",
    "sns.distplot( df[\"NumberOfTime60-89DaysPastDueNotWorse\"] , color=\"magenta\", ax=axes[2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by a subset of 6 variables, they are almost all largely skewed to the right, meaning that there are many outliers present in the data. We will now remove rows containing an outlier in any of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before: summarize the number of rows and columns in the dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = np.abs(stats.zscore(df.iloc[:,1:11]))\n",
    "print(zscore)\n",
    "threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show indexes of outliers\n",
    "print(np.where(zscore >3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zscore[0][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rcount outliers as anythign greater than 2.5 zscores from the mean, as this is a common rule of thumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full = df\n",
    "df_full = df_full[(zscore < 2.5).all(axis=1)]\n",
    "print(df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full['SeriousDlqin2yrs'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 12,226‬‬ outlier rows were removed. Let's see if this improves our histograms a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 2, figsize=(10, 10), sharex=False)\n",
    "sns.distplot( df_full[\"NumberOfDependents\"] , color=\"olive\", ax=axes[0, 0])\n",
    "sns.distplot( df_full[\"NumberOfTime30-59DaysPastDueNotWorse\"] , color=\"green\", ax=axes[0, 1])\n",
    "sns.distplot( df_full[\"NumberOfOpenCreditLinesAndLoans\"] , color=\"red\", ax=axes[1, 0])\n",
    "sns.distplot( df_full[\"NumberOfTimes90DaysLate\"] , color=\"blue\", ax=axes[1, 1])\n",
    "sns.distplot( df_full[\"NumberRealEstateLoansOrLines\"] , color=\"teal\", ax=axes[2, 0])\n",
    "sns.distplot( df_full[\"NumberOfTime60-89DaysPastDueNotWorse\"] , color=\"magenta\", ax=axes[2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is certainly improvement! We will need to apply transforms to ensure normal distributions for regression. It is perfectly okay that the distributions are still skewed, now the outlier values have just been removed.\n",
    "\n",
    "Now we have a dataset void of outliers and missing values - let's move onto visualizations and more preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Standardization/Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms, such as Neural Networks, require data to be consolidated into a standard range, such as 0-1. I will create a standard range dataset using min/max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_df = scaler.fit_transform(df_full)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=['SeriousDlqin2yrs','RevolvingUtilizationOfUnsecuredLines','age','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','NumberOfOpenCreditLinesAndLoans','NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines','NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(df_full['NumberOfTimes90DaysLate'], ax=ax1)\n",
    "sns.kdeplot(df_full['NumberRealEstateLoansOrLines'], ax=ax1)\n",
    "sns.kdeplot(df_full['age'], ax=ax1)\n",
    "ax2.set_title('After Min-Max Scaling')\n",
    "sns.kdeplot(scaled_df['NumberOfTimes90DaysLate'], ax=ax2)\n",
    "sns.kdeplot(scaled_df['NumberRealEstateLoansOrLines'], ax=ax2)\n",
    "sns.kdeplot(scaled_df['age'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all data points are scaled to the 0-1 range!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms, such as Regression, require data to be normally distributed. I will create this by applying various transforms to the features such as log, square root, or cube root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full2 = df_full[['RevolvingUtilizationOfUnsecuredLines','age','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','NumberOfOpenCreditLinesAndLoans','NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines','NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents']]\n",
    "bc = PowerTransformer(method='box-cox', standardize=False)\n",
    "yj = PowerTransformer(method='yeo-johnson', standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_df = yj.fit_transform(df_full2)\n",
    "norm_df = pd.DataFrame(norm_df, columns=['RevolvingUtilizationOfUnsecuredLines','age','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','NumberOfOpenCreditLinesAndLoans','NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines','NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.merge(df_full[['SeriousDlqin2yrs']], norm_df, left_index=True, right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 4, figsize=(10, 10), sharex=False)\n",
    "sns.distplot( df_full2[\"NumberOfDependents\"] , color=\"olive\", ax=axes[0, 0])\n",
    "sns.distplot( df_full2[\"NumberOfTime30-59DaysPastDueNotWorse\"] , color=\"green\", ax=axes[0, 1])\n",
    "sns.distplot( df_full2[\"NumberOfOpenCreditLinesAndLoans\"] , color=\"red\", ax=axes[1, 0])\n",
    "sns.distplot( df_full2[\"NumberOfTimes90DaysLate\"] , color=\"blue\", ax=axes[1, 1])\n",
    "sns.distplot( df_full2[\"NumberRealEstateLoansOrLines\"] , color=\"teal\", ax=axes[2, 0])\n",
    "sns.distplot( df_full2[\"NumberOfTime60-89DaysPastDueNotWorse\"] , color=\"magenta\", ax=axes[2, 1])\n",
    "sns.distplot( df_full2[\"MonthlyIncome\"] , color=\"firebrick\", ax=axes[0, 2])\n",
    "sns.distplot( df_full2[\"age\"] , color=\"darkorange\", ax=axes[1, 2])\n",
    "sns.distplot( df_full2[\"RevolvingUtilizationOfUnsecuredLines\"] , color=\"darkorchid\", ax=axes[2, 2])\n",
    "sns.distplot( df_full2[\"DebtRatio\"] , color=\"goldenrod\", ax=axes[0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 4, figsize=(10, 10), sharex=False)\n",
    "sns.distplot( norm_df[\"NumberOfDependents\"] , color=\"olive\", ax=axes[0, 0])\n",
    "sns.distplot( norm_df[\"NumberOfTime30-59DaysPastDueNotWorse\"] , color=\"green\", ax=axes[0, 1])\n",
    "sns.distplot( norm_df[\"NumberOfOpenCreditLinesAndLoans\"] , color=\"red\", ax=axes[1, 0])\n",
    "sns.distplot( norm_df[\"NumberOfTimes90DaysLate\"] , color=\"blue\", ax=axes[1, 1])\n",
    "sns.distplot( norm_df[\"NumberRealEstateLoansOrLines\"] , color=\"teal\", ax=axes[2, 0])\n",
    "sns.distplot( norm_df[\"NumberOfTime60-89DaysPastDueNotWorse\"] , color=\"magenta\", ax=axes[2, 1])\n",
    "sns.distplot( norm_df[\"MonthlyIncome\"] , color=\"firebrick\", ax=axes[0, 2])\n",
    "sns.distplot( norm_df[\"age\"] , color=\"darkorange\", ax=axes[1, 2])\n",
    "sns.distplot( norm_df[\"RevolvingUtilizationOfUnsecuredLines\"] , color=\"darkorchid\", ax=axes[2, 2])\n",
    "sns.distplot( norm_df[\"DebtRatio\"] , color=\"goldenrod\", ax=axes[0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the yeo-johnson transform did imporve the data's normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=(['RevolvingUtilizationOfUnsecuredLines','age','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','NumberOfOpenCreditLinesAndLoans','NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines','NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents'])\n",
    "full_train_cols = pd.DataFrame(df_full, columns = columns)\n",
    "full_train_target = pd.DataFrame(df_full['SeriousDlqin2yrs'])\n",
    "#full, unscaled dataset split\n",
    "fullxTrain, fullxTest, fullyTrain, fullyTest = train_test_split(full_train_cols, full_train_target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_cols = pd.DataFrame(scaled_df, columns = columns)\n",
    "scaled_train_target = pd.DataFrame(scaled_df['SeriousDlqin2yrs'])\n",
    "#scaled dataset split\n",
    "scaledxTrain, scaledxTest, scaledyTrain, scaledyTest = train_test_split(scaled_train_cols, scaled_train_target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_cols = pd.DataFrame(norm_df, columns = columns)\n",
    "norm_train_target = pd.DataFrame(norm_df['SeriousDlqin2yrs'])\n",
    "#normal distributed dataset split\n",
    "normxTrain, normxTest, normyTrain, normyTest = train_test_split(norm_train_cols, norm_train_target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fullxTrain.shape, fullyTrain.shape)\n",
    "print(fullxTest.shape, fullyTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaledxTrain.shape, scaledyTrain.shape)\n",
    "print(scaledxTest.shape, scaledyTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normxTrain.shape, normyTrain.shape)\n",
    "print(normxTest.shape, normyTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fullxTrain.describe(), fullyTrain.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results\n",
    "print(fullxTest.describe(), fullyTest.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaledxTrain.describe(), scaledyTrain.describe())\n",
    "#print results\n",
    "print(scaledxTest.describe(), scaledyTest.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normxTrain.describe(), normyTrain.describe())\n",
    "#print results\n",
    "print(normxTest.describe(), normyTest.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-collinearity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_full.iloc[:,0:5], kind=\"scatter\", hue=\"SeriousDlqin2yrs\", markers=[\"o\", \"s\"], palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.r_[0, 5:10]\n",
    "sns.pairplot(df_full.iloc[:,r], kind=\"scatter\", hue=\"SeriousDlqin2yrs\", markers=[\"o\", \"s\"], palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normyTrain['SeriousDlqin2yrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.countplot(x='SeriousDlqin2yrs', data=normyTrain, palette ='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is largely non-defaulted loans, but hopefully we can draw some strong conclusions/patterns from the 6457 defaulted loans (\"1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_default = len(normyTrain[normyTrain['SeriousDlqin2yrs']==1])\n",
    "count_no_default = len(normyTrain[normyTrain['SeriousDlqin2yrs']==0])\n",
    "pct_of_default = count_default/(count_default+count_no_default)\n",
    "print(\"percentage of default\", pct_of_default*100)\n",
    "pct_of_no_default = count_no_default/(count_default+count_no_default)\n",
    "print(\"percentage of no default\", pct_of_no_default*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_df.groupby('SeriousDlqin2yrs').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(normxTrain, normyTrain.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like every variable is considered a strong predictor, according to Recursive Feature Elimination (RFE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = ['RevolvingUtilizationOfUnsecuredLines','age','DebtRatio','MonthlyIncome','NumberOfOpenCreditLinesAndLoans','NumberRealEstateLoansOrLines','NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents']\n",
    "X = normxTrain[cols2]\n",
    "y = normyTrain['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(normxTrain,normyTrain.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(normxTest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(normxTest, normyTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(normyTest, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of All Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
